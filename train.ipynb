{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "from functions import image_segmentation_generator\n",
    "\n",
    "def train(model,\n",
    "          train_images,\n",
    "          train_annotations,\n",
    "          val_images,\n",
    "          val_annotations,\n",
    "          input_height=None,\n",
    "          input_width=None,\n",
    "          n_classes=None,\n",
    "          checkpoints_path = \"checkpoints\",\n",
    "          epochs=5,\n",
    "          batch_size=32,\n",
    "          steps_per_epoch=512,\n",
    "          val_steps_per_epoch=512,\n",
    "          load_weights=None,\n",
    "          read_image_type=1):  # cv2.IMREAD_COLOR = 1 (rgb)\n",
    "    \n",
    "    os.environ['PYTHONIOENCODING'] = 'utf-8'\n",
    "    n_classes = model.n_classes\n",
    "    input_height = model.input_height\n",
    "    input_width = model.input_width\n",
    "    output_height = model.output_height\n",
    "    output_width = model.output_width\n",
    "    \n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "\n",
    "    # Load weights if provided\n",
    "    # if load_weights:\n",
    "    #     print(f\"Loading weights from {load_weights}\")\n",
    "    #     model.load_weights(load_weights)\n",
    "    \n",
    "    # Save model configuration\n",
    "    # if checkpoints_path:\n",
    "    #     config_file = os.path.join(checkpoints_path, \"config.json\")\n",
    "    #     os.makedirs(os.path.dirname(config_file), exist_ok=True)\n",
    "    #     with open(config_file, \"w\") as f:\n",
    "    #         json.dump({\n",
    "    #             \"model_class\": model.model_name,\n",
    "    #             \"n_classes\": n_classes,\n",
    "    #             \"input_height\": input_height,\n",
    "    #             \"input_width\": input_width,\n",
    "    #             \"output_height\": model.output_height,\n",
    "    #             \"output_width\": model.output_width\n",
    "    #         }, f)\n",
    "    \n",
    "    train_gen = image_segmentation_generator(train_images, train_annotations, batch_size, n_classes,\n",
    "                                             input_height, input_width, model.output_height, model.output_width,\n",
    "                                             read_image_type=read_image_type)\n",
    "    \n",
    "    val_gen = image_segmentation_generator(val_images, val_annotations, batch_size, n_classes,\n",
    "                                           input_height, input_width, model.output_height, model.output_width,\n",
    "                                           read_image_type=read_image_type)\n",
    "    data, labels = next(train_gen)\n",
    "    print(\"Data batch shape:\", data.shape)\n",
    "    print(\"Labels batch shape:\", labels.shape)\n",
    "    \n",
    "    model.fit(train_gen, \n",
    "              steps_per_epoch=steps_per_epoch, \n",
    "              epochs=epochs,\n",
    "              validation_data=val_gen, \n",
    "              validation_steps=val_steps_per_epoch, verbose =1)\n",
    "    \n",
    "    # if checkpoints_path:\n",
    "    #     weights_path = os.path.join(checkpoints_path, \"model_weights.h5\")\n",
    "    #     print(f\"Saving model weights to {weights_path}\")\n",
    "    #     model.save_weights(weights_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "175\n",
      "43\n"
     ]
    }
   ],
   "source": [
    "from model import fcn_8_vgg\n",
    "batch_size = 32\n",
    "train_images = \"training_data/train_images\"\n",
    "train_annotations = \"training_data/train_annotations\"\n",
    "val_images = \"training_data/val_images\"\n",
    "val_annotations = \"training_data/val_annotations\"\n",
    "checkpoints_path = \"checkpoints\"\n",
    "steps_per_epoch= len(os.listdir(train_annotations)) // batch_size\n",
    "val_steps_per_epoch=len(os.listdir(val_images)) // batch_size\n",
    "print(steps_per_epoch)\n",
    "print(val_steps_per_epoch)\n",
    "n_classes = 27\n",
    "input_height = 224\n",
    "input_width = 320\n",
    "epochs = 5\n",
    "load_weights = None \n",
    "\n",
    "model = fcn_8_vgg(n_classes=n_classes, input_height=input_height, input_width=input_width)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data batch shape: (32, 224, 320, 3)\n",
      "Labels batch shape: (32, 76096, 27)\n",
      "Epoch 1/5\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m7638s\u001b[0m 44s/step - accuracy: 0.3811 - loss: 12.4670 - val_accuracy: 0.6546 - val_loss: 1.1846\n",
      "Epoch 2/5\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8301s\u001b[0m 47s/step - accuracy: 0.6654 - loss: 1.1590 - val_accuracy: 0.7043 - val_loss: 1.0260\n",
      "Epoch 3/5\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8414s\u001b[0m 48s/step - accuracy: 0.7068 - loss: 1.0150 - val_accuracy: 0.7266 - val_loss: 0.9389\n",
      "Epoch 4/5\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8410s\u001b[0m 48s/step - accuracy: 0.7253 - loss: 0.9395 - val_accuracy: 0.7385 - val_loss: 0.8814\n",
      "Epoch 5/5\n",
      "\u001b[1m175/175\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8653s\u001b[0m 49s/step - accuracy: 0.7378 - loss: 0.8816 - val_accuracy: 0.7427 - val_loss: 0.8499\n"
     ]
    }
   ],
   "source": [
    "train(\n",
    "    model=model,\n",
    "    train_images=train_images,\n",
    "    train_annotations=train_annotations,\n",
    "    val_images=val_images,\n",
    "    val_annotations=val_annotations,\n",
    "    epochs=epochs,\n",
    "    batch_size=batch_size,\n",
    "    steps_per_epoch=steps_per_epoch,\n",
    "    val_steps_per_epoch=val_steps_per_epoch,\n",
    "    load_weights=load_weights\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving model weights to checkpoints\\model.weights.h5\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "weights_path = os.path.join(\"checkpoints\", \"model.weights.h5\")\n",
    "print(f\"Saving model weights to {weights_path}\")\n",
    "model.save_weights(weights_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
